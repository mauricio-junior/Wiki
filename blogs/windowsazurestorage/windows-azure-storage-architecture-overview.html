---
layout: windowsazurestorage
title: Windows Azure Storage Architecture Overview
weight: 3
---

<div id="site-content" class="site-content row">

	<div id="primary" class="content-area col-sm-9">
		<div id="single-content" class="div-content">

			
				
<article id="post-48" class="post-48 post type-post status-publish format-standard hentry category-uncategorized tag-architecture tag-scalability">

	<header class="entry-header single">
		<h1 class="entry-title">Windows Azure Storage Architecture Overview</h1>		<div class="rating-wrap">
		<div id="star-rating-48" class="wds-ratings" data-rating="4" data-userrating="0" data-postid="48" data-container="body" data-toggle="tooltip" title="4 user(s) rated">
			<div class="wds-ratings-inner-wrap">
				<div>
					<div aria-label="Select 5 star, this article is 4 star rated" tabindex="0" class="wds-ratings-stars wds-ratings-stars-enable-editing" data-stars="5"><span aria-hidden="true" class="star-1"><span>&#x2605;</span></span><span aria-hidden="true" class="star-2"><span>&#x2605;</span></span><span aria-hidden="true" class="star-3"><span>&#x2605;</span></span><span aria-hidden="true" class="star-4"><span>&#x2605;</span></span><span aria-hidden="true" class="star-5"><span>&#x2605;</span></span></div><div aria-label="Select 4 star, this article is 4 star rated" tabindex="0" class="wds-ratings-stars wds-ratings-stars-enable-editing" data-stars="4"><span aria-hidden="true" class="star-1"><span>&#x2605;</span></span><span aria-hidden="true" class="star-2"><span>&#x2605;</span></span><span aria-hidden="true" class="star-3"><span>&#x2605;</span></span><span aria-hidden="true" class="star-4"><span>&#x2605;</span></span></div><div aria-label="Select 3 star, this article is 4 star rated" tabindex="0" class="wds-ratings-stars wds-ratings-stars-enable-editing" data-stars="3"><span aria-hidden="true" class="star-1"><span>&#x2605;</span></span><span aria-hidden="true" class="star-2"><span>&#x2605;</span></span><span aria-hidden="true" class="star-3"><span>&#x2605;</span></span></div><div aria-label="Select 2 star, this article is 4 star rated" tabindex="0" class="wds-ratings-stars wds-ratings-stars-enable-editing" data-stars="2"><span aria-hidden="true" class="star-1"><span>&#x2605;</span></span><span aria-hidden="true" class="star-2"><span>&#x2605;</span></span></div><div aria-label="Select 1 star, this article is 4 star rated" tabindex="0" class="wds-ratings-stars wds-ratings-stars-enable-editing" data-stars="1"><span aria-hidden="true" class="star-1"><span>&#x2605;</span></span></div>
				</div>
			</div>
		</div>
		</div>		<div class="clear-both"></div>
		<div class="entry-meta">
			<img alt="avatar of windows-azure-storage" src="https://i1.social.s-msft.com/profile/u/avatar.jpg?displayname=Windows+Azure+Storage&amp;size=extralarge&amp;version=00000000-0000-0000-0000-000000000000" class="avatar avatar-22 photo" height="22" width="22"/><span class="byline"><span class="author vcard"><a class="url fn n profile-usercard-hover" data-profile-userid="59e1aedafa2d4428b7004b2c2fac4adc" href="https://social.msdn.microsoft.com/profile/Windows+Azure+Storage">Windows Azure Storage</a></span></span><span class="posted-on posted-on-margin"><span class="screen-reader-text"></span><time class="entry-date published updated" datetime="2010-12-30T14:48:00+00:00">December 30, 2010</time></span><span class="comments-link"><span class="glyphicon glyphicon-comment" aria-hidden="true"></span><a href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/#comments">3</a></span>				<span class="social-icons-wrap">
		<ul class="social-icons">
			<li><div class="fb-share-button" data-href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/" data-layout="button_count" data-size="large" data-mobile-iframe="true"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/&amp;src=sdkpreparse">Share</a></div></li>
			<div id="fb-root" style="display:none"></div>

			<li class="social-icon twitter"><a data-social="{&quot;type&quot;:&quot;twitter&quot;, &quot;url&quot;:&quot;https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/&quot;, &quot;text&quot;: &quot;Windows Azure Storage Architecture Overview&quot;}" href="#" id="post_tweet_count">0</a></li>
			<li class="social-icon linkedin"><a data-social="{&quot;type&quot;:&quot;linkedin&quot;, &quot;url&quot;:&quot;https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/&quot;, &quot;text&quot;: &quot;Windows Azure Storage Architecture Overview&quot;}" href="#" id="get_post_linkedin_count">0</a></li>
		</ul>
	</span><!-- .social-icons-wrap -->
	<script type="text/javascript">
		// Get social counts
		jQuery( window ).load(function () {
			jQuery.getScript('https://blogs.msdn.microsoft.com/windowsazurestorage/wp-content/themes/microsoft/js/social-counts.js?ver=02092017')
				.done(function(script,textStatus) {
					window.msdnsocial.ajax('https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/');
				});
		});
	</script>
		</div>
		<hr>
	</header><!-- .entry-header -->

	<div class="entry-content single">
		<p><span style="font-size: medium;"><em><strong></strong></em></span>&nbsp;</p>
<p><span style="font-size: medium;"><strong>Update 1-2-2012:&nbsp; See the new post on <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2011/11/20/windows-azure-storage-a-highly-available-cloud-storage-service-with-strong-consistency.aspx">Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency </a>, which gives a much more detailed and up to date description of the Windows Azure Storage Architecture.</strong></span></p>
<p>&nbsp;</p>
<p>In this posting we provide an overview of the Windows Azure Storage architecture to give some understanding of how it works. Windows Azure Storage is a distributed storage software stack built completely by Microsoft for the cloud.</p>
<p>Before diving into the details of this post, please read the prior posting on <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/05/10/windows-azure-storage-abstractions-and-their-scalability-targets.aspx"><i>Windows Azure Storage Abstractions and their Scalability Targets</i></a> to get an understanding of the storage abstractions (Blobs, <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/11/06/how-to-get-most-out-of-windows-azure-tables.aspx">Tables</a> and Queues) provided and the concept of partitions.</p>
<h4><b>3 Layer Architecture</b></h4>
<p>The storage access architecture has the following 3 fundamental layers:</p>
<ol>
<li><b>Front-End (FE) layer</b> &ndash; This layer takes the incoming requests, authenticates and authorizes the requests, and then routes them to a partition server in the Partition Layer. The front-ends know what partition server to forward each request to, since each front-end server caches a Partition Map. The Partition Map keeps track of the partitions for the service being accessed (Blobs, Tables or Queues) and what partition server is controlling (serving) access to each partition in the system.</li>
<li><b>Partition Layer</b> &ndash; This layer manages the partitioning of all of the data objects in the system. As described in the <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/05/10/windows-azure-storage-abstractions-and-their-scalability-targets.aspx">prior posting</a>, all objects have a partition key. An object belongs to a single partition, and each partition is served by only one partition server. This is the layer that manages what partition is served on what partition server. In addition, it provides automatic load balancing of partitions across the servers to meet the traffic needs of Blobs, Tables and Queues. A single partition server can serve many partitions.</li>
<li><b>Distributed and replicated File System (DFS) Layer</b> &ndash; This is the layer that actually stores the bits on disk and is in charge of distributing and replicating the data across many servers to keep it durable. A key concept to understand here is that the data is stored by the DFS layer, but all DFS servers are (and all data stored in the DFS layer is) accessible from any of the partition servers.</li>
</ol>
<p>These layers and a high level overview are shown in the below figure:</p>
<p><a href="https://msdnshared.blob.core.windows.net/media/MSDNBlogsFS/prod.evol.blogs.msdn.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/36/55/metablogapi/0181.image_79D0B3C2.png" original-url="http://blogs.msdn.com/cfs-file.ashx/__key/CommunityServer-Blogs-Components-WeblogFiles/00-00-01-36-55-metablogapi/0181.image_5F00_79D0B3C2.png"><img style="display: inline; border-width: 0px;" title="image" border="0" alt="image" src="https://msdnshared.blob.core.windows.net/media/MSDNBlogsFS/prod.evol.blogs.msdn.com/CommunityServer.Blogs.Components.WeblogFiles/00/00/01/36/55/metablogapi/6837.image_thumb_228C190C.png" original-url="http://blogs.msdn.com/cfs-file.ashx/__key/CommunityServer-Blogs-Components-WeblogFiles/00-00-01-36-55-metablogapi/6837.image_5F00_thumb_5F00_228C190C.png" width="484" height="364"/></a></p>
<p>Here we can see that the Front-End layer takes incoming requests, and a given front-end server can talk to all of the partition servers it needs to in order to process the incoming requests. The partition layer consists of all of the partition servers, with a master system to perform the automatic load balancing (described below) and assignments of partitions. As shown in the figure, each partition server is assigned a set of object partitions (Blobs, Entities, Queues). The Partition Master constantly monitors the overall load on each partition sever as well the individual partitions, and uses this for load balancing. Then the lowest layer of the storage architecture is the Distributed File System layer, which stores and replicates the data, and all partition servers can access any of the DFS severs.</p>
<h4><b>Lifecycle of a Request</b></h4>
<p>To understand how the architecture works, let&rsquo;s first go through the lifecycle of a request as it flows through the system. The process is the same for Blob, Entity and Message requests:</p>
<ol>
<li><b>DNS lookup</b> &ndash; the request to be performed against Windows Azure Storage does a DNS resolution on the domain name for the object&rsquo;s Uri being accessed. For example, the domain name for a blob request is &ldquo;<i>&lt;your_account&gt;.</i>blob.core.windows.net&rdquo;. This is used to direct the request to the geo-location (sub-region)&nbsp;the storage account is assigned to, as well as to the blob service in that geo-location.</li>
<li><b>Front-End Server Processes Request</b>&ndash; The request reaches a front-end, which does the following:
<ol>
<li>Perform authentication and authorization for the request</li>
<li>Use the request&rsquo;s partition key to look up in the Partition Map to find which partition server is serving the partition. <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/05/10/windows-azure-storage-abstractions-and-their-scalability-targets.aspx">See this post for a description of a request&rsquo;s partition key.</a></li>
<li>Send the request to the corresponding partition server</li>
<li>Get the response from the partition server, and send it back to the client.</li>
</ol>
</li>
<li><b>Partition Server Processes Request</b>&ndash; The request arrives at the partition server, and the following occurs depending on whether the request is a GET (read operation) or a PUT/POST/DELETE (write operation):
<ul>
<li>GET &ndash; See if the data is cached in memory at the partition server
<ol>
<li>If so, return the data directly from memory.</li>
<li>Else, send a read request to one of the DFS Servers holding one of the replicas for the data being read.</li>
</ol>
</li>
<li>PUT/POST/DELETE
<ol>
<li>Send the request to the primary DFS Server (see below for details) holding the data to perform the insert/update/delete.</li>
</ol>
</li>
</ul>
</li>
<li><b>DFS Server Processes Request </b>&ndash; the data is read/inserted/updated/deleted from persistent storage and the status (and data if read) is returned. Note, for insert/update/delete, the data is replicated across multiple DFS Servers before success is returned back to the client (see below for details).</li>
</ol>
<p>Most requests are to a single partition, but listing Blob Containers, Blobs, Tables, and Queues, and Table Queries can span multiple partitions. When a listing/query request that spans partitions arrives at a FE server, we know via the Partition Map the set of partition servers that need to be contacted to perform the query. Depending upon the query and the number of partitions being queried over, the query may only need to go to a single partition server to process its request. If the Partition Map shows that the query needs to go to more than one partition server, we serialize the query by performing it across those partition servers one at a time sorted in partition key order. Then at partition server boundaries, or when we reach 1,000 results for the query, or when we reach 5 seconds of processing time, we return the results accumulated thus far and a continuation token if we are not yet done with the query. Then when the client passes the continuation token back in to continue the listing/query, we know the Primary Key from which to continue the listing/query.</p>
<h4><b>Fault Domains and Server Failures</b></h4>
<p>Now we want to touch on how we maintain availability in the face of hardware failures. The first concept is to spread out the servers across different fault domains, so if a hardware fault occurs only a small percentage of servers are affected. The servers for these 3 layers are broken up over different fault domains, so if a given fault domain (rack, network switch, power) goes down, the service can still stay available for serving data.</p>
<p>The following is how we deal with node failures for each of the three different layers:</p>
<ul>
<li><b>Front-End Server Failure</b> &ndash; If a front-end server becomes unresponsive, then the load balancer will realize this and take it out of the available servers that serve requests from the incoming VIP. This ensures that requests hitting the VIP get sent to live front-end servers that are waiting to process requests.</li>
<li><b>Partition Server Failure</b> &ndash; If the storage system determines that a partition server is unavailable, it immediately reassigns any partitions it was serving to other available partition servers, and the Partition Map for the front-end servers is updated to reflect this change (so front-ends can correctly locate the re-assigned partitions). Note, when assigning partitions to different partition servers no data is moved around on disk, since all of the partition data is stored in the DFS server layer and accessible from any partition server. The storage system ensures that all partitions are always served.</li>
<li><b>DFS Server Failure</b> &ndash; If the storage system determines a DFS server is unavailable, the partition layer stops using the DFS server for reading and writing while it is unavailable. Instead, the partition layer uses the other available DFS servers which contain the other replicas of the data. If a DFS Server is unavailable for too long, we generate additional replicas of the data in order to keep the&nbsp;data at a healthy number of replicats for durability.</li>
</ul>
<h4><b>Upgrade Domains and Rolling Upgrade</b></h4>
<p>A concept orthogonal to fault domains is what we call upgrade domains. Servers for each of the 3 layers are spread evenly across the different fault domains, and upgrade domains for the storage service. This way if a fault domain goes down we lose at most 1/X of the servers for a given layer, where X is the number of fault domains. Similarly, during a service upgrade at most 1/Y of the servers for a given layer are upgraded at a given time, where Y is the number of upgrade domains. To achieve this, we use rolling upgrades, which allows us to maintain high availability when upgrading the storage service.</p>
<p>The servers in each layer are broken up over a set of upgrade domains, and we upgrade a single upgrade domain at a time. For example, if we have 10 upgrade domains, then upgrading a single domain would potentially upgrade up to 10% of the servers from each layer at a time. A description of upgrade domains and an example of using rolling upgrades is in the <a href="http://microsoftpdc.com/Sessions/SVC08">PDC 2009 talk on Patterns for Building Scalable and Reliable Applications for Windows Azure (at 25:00)</a>.</p>
<p>We upgrade a single domain at a time for our storage service using rolling upgrades. A key part for maintaining availability during upgrade is that before upgrading a given domain, we proactively offload all the partitions being served on partition servers in that upgrade domain. In addition, we mark the DFS servers in that upgrade domain as being upgraded so they are not used while the upgrade is going on. This preparation is done before upgrading the domain, so that when we upgrade we reduce the impact on the service to maintain high availability.</p>
<p>After an upgrade domain has finished upgrading we allow the servers in that domain to serve data again. In addition, after we upgrade a given domain, we validate that everything is running fine with the service before going to the next upgrade domain. This process allows us to verify production configuration, above and beyond the pre-release testing we do, on just a small percentage of servers in the first few upgrade domains before upgrading the whole service. Typically if something is going to go wrong during an upgrade, it will occur when upgrading the first one or two upgrade domains, and if something doesn&rsquo;t look quite right we pause upgrade to investigate, and we can even rollback to the prior version of the production software if need be.</p>
<p>Now we will go through the lower to layers of our system in more detail, starting with the DFS Layer.</p>
<h4><b>DFS Layer and Replication </b></h4>
<p>Durability for Windows Azure Storage is provided through replication of your data, where all data is replicated multiple times. The underlying replication layer is a Distributed File System (DFS) with the data being spread out over hundreds of storage nodes. Since the underlying replication layer is a distributed file system, the replicas are accessible from all of the partition servers as well as from other DFS servers.</p>
<p>The DFS layer stores the data in what are called &ldquo;extents&rdquo;. This is the unit of storage on disk and unit of replication, where each extent is replicated multiple times. The typical extent sizes range from approximately 100MB to 1GB in size.</p>
<p>When storing a blob in a Blob Container, entities in a Table, or messages in a Queue, the persistent data is stored in one or more extents. Each of these extents has multiple replicas, which are spread out randomly over the different DFS servers providing &ldquo;Data Spreading&rdquo;. For example, a 10GB blob may be stored across 10 one-GB extents, and if there are 3 replicas for each extent, then the corresponding 30 extent replicas for this blob could be spread over 30 different DFS servers for storage. This design allows Blobs, Tables and Queues to span multiple disk drives and DFS servers, since the data is broken up into chunks (extents) and the DFS layer spreads the extents across many different DFS servers. This design also allows a higher number of IOps and network BW for accessing Blobs, Tables, and Queues as compared to the IOps/BW available on a single storage DFS server. This is a direct result of the data being spread over multiple extents, which are in turn spread over different disks and different DFS servers,&nbsp;since any of the replicas of an extent can be used for reading the data.</p>
<p>For a given extent, the DFS has a primary server and multiple secondary servers. All writes go through the primary server, which then sends the writes to the secondary servers. Success is returned back from the primary to the client once the data is written to at least 3 DFS servers. If one of the DFS servers is unreachable when doing the write, the DFS layer will choose more servers to write the data to so that (a) all data updates are written at least 3 times (3 separate disks/servers in 3 separate fault+upgrade domains) before returning success to the client and (b) writes can make forward progress in the face of a DFS server being unreachable. Reads can be processed from any up-to-date extent replica (primary or secondary), so reads can be successfully processed from the extent replicas on its secondary DFS servers.</p>
<p>The multiple replicas for an extent are spread over different fault domains and upgrade domains, therefore no two replicas for an extent will be placed in the same fault domain or upgrade domain. Multiple replicas are kept for each data item, so if one fault domain goes down, there will still be healthy replicas to access the data from, and the system will dynamically re-replicate the data to bring it back to a healthy number of replicas. During upgrades, each upgrade domain is upgraded separately, as described above. If an extent replica for your data is in one of the domains currently being upgraded, the extent data will be served from one of the currently available replicas in the other upgrade domains not being upgraded.</p>
<p>A key principle of the replication layer is dynamic re-replication and having a low MTTR (mean-time-to-recovery). If a given DFS server is lost or a drive fails, then all of the extents that had a replica on the lost node/drive are quickly re-replicated to get those extents back to a healthy number of replicas. Re-replication is accomplished quickly, since the other healthy replicas for the affected extents are randomly spread across the many DFS servers in different fault/upgrade domains, providing sufficient disk/network bandwidth to rebuild replicas very quickly. For example, to re-replicate a failed DFS server with many TBs of data, with potentially 10s of thousands of lost extent replicas, the healthy replicas for those extents are potentially spread across hundreds to thousands of storage nodes and drives. To get those extents back up to a healthy number of replicas, all of those storage nodes and drives&nbsp;can be used to (a) read from the healthy remaining replicas, and (b) write another copy of the lost replica to a random node in a different fault/upgrade domain for the extent. This recovery process allows us to leverage the available network/disk resources across all of the nodes in the storage service to potentially re-replicate a lost storage node within minutes, which is a key property to having a low MTTR in order to prevent data loss.</p>
<p>Another important property of the DFS replication layer is checking and scanning data for bit rot. All data written has a checksum (internal to the storage system) stored with it. The data is continually scanned for bit rot by reading the data and verifying the checksum. In addition, we always validate this internal checksum when reading the data for a client request. If an extent replica is found to be corrupt by one of these checks, then the corrupted replica is discarded and the extent&nbsp;is re-replicated using one of the valid replicas in order to bring the extent back to healthy level of replication.</p>
<h4><b>Geo-Replication</b></h4>
<p>Windows Azure Storage provides durability by constantly maintaining multiple healthy replicas for your data. To achieve this, replication is provided within a single location (e.g., US South), across different fault and upgrade domains as described above. This provides durability within a given location. But what if a location has a regional disaster (e.g., wild fire, earthquake, etc.) that can potentially affect an area for many miles?</p>
<p>We are working on providing a feature called geo-replication, which replicates customer data hundreds of miles between two locations (i.e., between North and South US, between North and West Europe, and between East and Southeast Asia) to provide disaster recovery in case of regional disasters. The geo-replication is in addition to the multiple copies maintained by the DFS layer within a single location described above. We will have more details in a future blog post on how geo-replication works and how it provides geo-diversity in order to provide disaster recovery if a regional disaster were to occur.</p>
<h4><b>Load Balancing Hot DFS Servers</b></h4>
<p>Windows Azure Storage has load balancing at the partition layer and also at the DFS layer. The partition load balancing addresses the issue of a partition server getting too many requests per second for it to handle for the partitions it is serving, and load balancing those partitions across other partition servers to even out the load. The DFS layer is instead focused on load balancing the I/O load to its disks and the network BW to its servers.</p>
<p>The DFS servers can get too hot in terms of the I/O and BW load, and we provide automatic load balancing for DFS servers to address this. We provide two forms of load balancing at the DFS layer:</p>
<ul>
<li><b>Read Load Balancing</b> - The DFS layer maintains multiple copies of data through the multiple replicas it keeps, and the system is built to allow reading from any of the up to date replica copies. The system keeps track of the load on the DFS servers. If a DFS server is getting too many requests for it to handle, partition servers trying to access that DFS server will be routed to read from other DFS servers that are holding replicas of the data the partition server is trying to access. This effectively load balances the reads across DFS servers when a given DFS server gets too hot. If all of the DFS servers are too hot for a given set of data accessed from partition servers, we have the option to increase the number of copies of the data in the DFS layer to provide more throughput. However, hot data is&nbsp;mostly handled by the partition layer, since the partition layer caches hot data,&nbsp;and hot data is served directly from the partition server cache without going to the DFS layer.</li>
<li><b>Write Load Balancing</b> &ndash; All writes to a given piece of data go to a primary DFS server, which coordinates the writes to the secondary DFS servers for the extent. If any of the DFS servers becomes too hot to service the requests, the storage system will then choose&nbsp;different DFS servers to write the data to.</li>
</ul>
<h4><b>Why Both a Partition Layer and DFS Layer?</b></h4>
<p>When describing the architecture, one question we get is why do we have both a Partition layer and a DFS layer, instead of just one layer both storing the data and providing load balancing?</p>
<p>The DFS layer can be thought of as our file system layer, it understand files (these large chunks of storage called extents), how to store them, how to replicate them, etc, but it doesn&rsquo;t understand higher level object constructs nor their semantics. The partition layer is built specifically for managing and understanding higher level data abstractions, and storing them on top of the DFS.</p>
<p>The partition layer understands what a transaction means for a given object type (Blobs, Entities, Messages). In addition, it provides the ordering of parallel transactions and strong consistency for the different types of objects. Finally, the partition layer spreads large objects across multiple DFS server chunks (called extents) so that large objects (e.g., 1 TB Blobs) can be stored without having to worry about running out of space on a single disk or DFS server, since a large blob is spread out over many DFS servers and disks.</p>
<h4><b>Partitions and Partition Servers</b></h4>
<p>When we say that a partition server is serving a partition, we mean that the partition server has been designated as the server (for the time being) that controls all access to the objects in that partition. We do this so that for a given set of objects there is a single server ordering transactions to those objects and providing strong consistency and optimistic concurrency, since a single server is in control of the access of a given partition of objects.</p>
<p>In the prior <a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/05/10/windows-azure-storage-abstractions-and-their-scalability-targets.aspx">scalability targets post</a> we described that a single partition can process up to 500 entities/messages per second. This is because all of the requests to a single partition have to be served by the assigned partition server. Therefore, it is important to understand the scalability targets and the partition keys for Blobs, Tables and Queues when designing your solutions (<i>see the upcoming posts focused on getting the most out of Blobs, </i><a href="http://blogs.msdn.com/b/windowsazurestorage/archive/2010/11/06/how-to-get-most-out-of-windows-azure-tables.aspx"><i>Tables</i></a><i> and Queues for more information</i>).</p>
<h4><b>Load Balancing Hot Partition Servers</b></h4>
<p>It is important to understand that partitions are not tied to specific partition servers, since the data is stored in the DFS layer. The partition layer can therefore easily load balance and assign partitions to different partition servers, since any partition server can potentially provide access to any partition.</p>
<p>The partition layer assigns partitions to partition severs based on each partition&rsquo;s load. A given partition server may serve many partitions, and the Partition Master continuously monitors the load on all partition servers. If it sees that a partition server has too much load, the partition layer will automatically load balance some of the partitions from that partition server to a partition server with low load.</p>
<p>When reassigning a partition from one partition server to another, the partition is offline only for a handful seconds, in order to maintain high availability for the partition. Then in order to make sure we do not move partitions around too much and make too quick of decisions, the time it takes to decide to load balance a hot partition server is on the order of minutes.</p>
<h4><b>Summary</b></h4>
<p>The Windows Azure Storage architecture had three main layers &ndash; Front-End layer, Partition layer, and DFS layer. For availability, each layer has its own form of automatic load balancing and dealing with failures and recovery in order to provide high availability when accessing your data. For durability, this is provided by the DFS layer keeping multiple replicas of your data and using data spreading to keep a low MTTR when failures occur. For consistency, the partition layer provides strong consistency and optimistic concurrency by making sure a single partition server is always ordering and serving up access to each of your data partitions.</p>
<p>Brad Calder</p>
	</div><!-- .entry-content -->


	<footer class="entry-footer single">
					<div class="tags">
				<span>Tags </span>
				<span>
					<a href="https://blogs.msdn.microsoft.com/windowsazurestorage/tag/architecture/" rel="tag">Architecture</a> <a href="https://blogs.msdn.microsoft.com/windowsazurestorage/tag/scalability/" rel="tag">Scalability</a>				</span>
			</div>
				<hr>
	</footer><!-- .entry-footer -->

</article><!-- #post-## -->

			
		</div><!-- #single-content -->

		
<div id="comments" class="comments-area">
	<div class="comments-title">
		Comments (3)	</div>

		<div id="respond" class="comment-respond">
		<h3 id="reply-title" class="comment-reply-title"> <small><a rel="nofollow" id="cancel-comment-reply-link" href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/#respond" style="display:none;">Cancel reply</a></small></h3><p class="must-log-in">You must be <a href="https://blogs.msdn.microsoft.com/windowsazurestorage/wp-login.php?redirect_to=https%3A%2F%2Fblogs.msdn.microsoft.com%2Fwindowsazurestorage%2F2010%2F12%2F30%2Fwindows-azure-storage-architecture-overview%2F">logged in</a> to post a comment.</p>	</div><!-- #respond -->
	
			<div class="navigation pagination clear-both">
					</div>

		<ol class="comment-list">
					<li class="comment even thread-even depth-1" id="comment-2771">
				<div id="div-comment-2771" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="https://secure.gravatar.com/avatar/ece51e5accc62dfc57ded89ea50764be?s=56&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/ece51e5accc62dfc57ded89ea50764be?s=112&amp;d=mm&amp;r=g 2x" class="avatar avatar-56 photo" height="56" width="56"/>			<cite class="fn"><a href="http://blogs.msdn.com/moyhajjat_4000_hotmail.com/ProfileUrlRedirect.ashx" rel="external nofollow" class="url">moyhajjat@hotmail.com</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/#comment-2771">
			May 4, 2011 at 7:21 am</a>		</div>

		<p>Hi, how can we achieve data consistency across data-centers? assume there are two populations of users in two locations A and B, and it would be faster for each population to go the nearby data-center. Assume each employee record is a single blob. Is there a way to keep data consistent if multiple writes/updates happen on the same employee record but on two different data-centers? or do I have to implement my own logic for that?</p>
<p>Thanks,</p>
<p>Mohammad Hajjat</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-login" href="https://blogs.msdn.microsoft.com/windowsazurestorage/wp-login.php?redirect_to=https%3A%2F%2Fblogs.msdn.microsoft.com%2Fwindowsazurestorage%2F2010%2F12%2F30%2Fwindows-azure-storage-architecture-overview%2F">Log in to Reply</a></div>
				</div>
		</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1" id="comment-2761">
				<div id="div-comment-2761" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="https://secure.gravatar.com/avatar/a79a1b675acd535552dd5817613979ab?s=56&amp;d=mm&amp;r=g" srcset="https://secure.gravatar.com/avatar/a79a1b675acd535552dd5817613979ab?s=112&amp;d=mm&amp;r=g 2x" class="avatar avatar-56 photo" height="56" width="56"/>			<cite class="fn"><a href="http://blogs.msdn.com/Brad-Calder-_2800_MSFT_2900_/ProfileUrlRedirect.ashx" rel="external nofollow" class="url">shop.calder@hotmail.com</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/#comment-2761">
			May 10, 2011 at 10:09 pm</a>		</div>

		<p>@Mohammad</p>
<p>If you want to have consistent writes across multiple data centers, then that has to be done at the application level. &nbsp;The geo-replication referred to above is asynchronous, where the update is first committed at the primary data center and success returned back to the client. Then in the background the update is sent from the primary to the secondary data center. &nbsp;I’ll have more information posted about geo-replication by the end of the year.</p>
<p>Brad</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-login" href="https://blogs.msdn.microsoft.com/windowsazurestorage/wp-login.php?redirect_to=https%3A%2F%2Fblogs.msdn.microsoft.com%2Fwindowsazurestorage%2F2010%2F12%2F30%2Fwindows-azure-storage-architecture-overview%2F">Log in to Reply</a></div>
				</div>
		</li><!-- #comment-## -->
		<li class="comment byuser comment-author-srigopal_saihotmail-com even thread-even depth-1" id="comment-2741">
				<div id="div-comment-2741" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="https://i1.social.s-msft.com/profile/u/avatar.jpg?displayname=Srigopal+Chitrapu&amp;size=extralarge&amp;version=fa441690-4928-498a-b1e5-5b82144c2240" class="avatar avatar-56 photo" height="56" width="56"/>			<cite class="fn"><a href="https://social.msdn.microsoft.com/profile/Srigopal+Chitrapu" rel="external nofollow" class="url">Srigopal Chitrapu</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata"><a href="https://blogs.msdn.microsoft.com/windowsazurestorage/2010/12/30/windows-azure-storage-architecture-overview/#comment-2741">
			December 1, 2012 at 4:07 pm</a>		</div>

		<p>Nice Info.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-login" href="https://blogs.msdn.microsoft.com/windowsazurestorage/wp-login.php?redirect_to=https%3A%2F%2Fblogs.msdn.microsoft.com%2Fwindowsazurestorage%2F2010%2F12%2F30%2Fwindows-azure-storage-architecture-overview%2F">Log in to Reply</a></div>
				</div>
		</li><!-- #comment-## -->
		</ol><!-- .comment-list -->

		<div class="navigation pagination">
					</div>

	
	
</div><!-- .comments-area -->
	</div><!-- #primary -->